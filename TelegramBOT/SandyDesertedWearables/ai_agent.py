
# ai_agent.py - ÙˆÙƒÙŠÙ„ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø§Ù„Ù…ØªØ·ÙˆØ±
import os
import json
import requests
from typing import Dict, List, Any, Optional
from langchain.agents import AgentExecutor, create_openai_tools_agent
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.memory import ConversationBufferWindowMemory
from tools import ALL_TOOLS
from dotenv import load_dotenv

load_dotenv()

class MultiModelAIManager:
    """Ù…Ø¯ÙŠØ± Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…ØªØ¹Ø¯Ø¯Ø© Ù„Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ"""
    
    def __init__(self):
        self.available_models = {
            'openai': {
                'models': ['gpt-4', 'gpt-3.5-turbo', 'gpt-4-turbo'],
                'api_key_env': 'OPENAI_API_KEY',
                'base_url': 'https://api.openai.com/v1'
            },
            'google': {
                'models': ['gemini-2.0-flash', 'gemini-2.5-pro', 'gemini-2.5-flash', 'gemini-2.5-flash-lite'],
                'api_key_env': 'GEMINI_API_KEY',
                'base_url': 'https://generativelanguage.googleapis.com/v1beta'
            },
            'openrouter': {
                'models': ['claude-3-opus', 'llama-2-70b', 'mistral-7b'],
                'api_key_env': 'OPENROUTER_API_KEY',
                'base_url': 'https://openrouter.ai/api/v1'
            },
            'anthropic': {
                'models': ['claude-3-opus', 'claude-3-sonnet', 'claude-3-haiku'],
                'api_key_env': 'ANTHROPIC_API_KEY',
                'base_url': 'https://api.anthropic.com/v1'
            }
        }
        
        self.active_model = None
        self.active_provider = None
    
    def get_available_providers(self) -> Dict[str, List[str]]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙˆÙØ±ÙŠÙ† ÙˆØ§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…ØªØ§Ø­Ø©"""
        available = {}
        for provider, config in self.available_models.items():
            api_key = os.getenv(config['api_key_env'])
            if api_key:
                available[provider] = config['models']
        return available
    
    def set_active_model(self, provider: str, model: str) -> Dict[str, Any]:
        """ØªØ¹ÙŠÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù†Ø´Ø·"""
        if provider not in self.available_models:
            return {'success': False, 'error': f'Ù…ÙˆÙØ± ØºÙŠØ± Ù…Ø¯Ø¹ÙˆÙ…: {provider}'}
        
        config = self.available_models[provider]
        api_key = os.getenv(config['api_key_env'])
        
        if not api_key:
            return {'success': False, 'error': f'Ù…ÙØªØ§Ø­ API ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯ Ù„Ù€ {provider}'}
        
        if model not in config['models']:
            return {'success': False, 'error': f'Ù†Ù…ÙˆØ°Ø¬ ØºÙŠØ± Ù…Ø¯Ø¹ÙˆÙ…: {model}'}
        
        self.active_provider = provider
        self.active_model = model
        
        return {'success': True, 'message': f'ØªÙ… ØªØ¹ÙŠÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: {provider}/{model}'}
    
    def create_llm_instance(self):
        """Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø«ÙŠÙ„ LLM Ø­Ø³Ø¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø®ØªØ§Ø±"""
        if not self.active_provider or not self.active_model:
            return None
        
        config = self.available_models[self.active_provider]
        api_key = os.getenv(config['api_key_env'])
        
        if self.active_provider == 'google':
            return self._create_gemini_instance(api_key)
        else:
            return ChatOpenAI(
                model=self.active_model,
                openai_api_key=api_key,
                openai_api_base=config['base_url'],
                temperature=0.7
            )
    
    def _create_gemini_instance(self, api_key: str):
        """Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø«ÙŠÙ„ Gemini Ù…Ø®ØµØµ"""
        class GeminiWrapper:
            def __init__(self, api_key: str, model: str):
                self.api_key = api_key
                self.model = model
                self.base_url = "https://generativelanguage.googleapis.com/v1beta"
            
            def invoke(self, messages):
                """Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Gemini API"""
                try:
                    # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø±Ø³Ø§Ø¦Ù„ Ø¥Ù„Ù‰ ØµÙŠØºØ© Gemini
                    content = []
                    for msg in messages:
                        if hasattr(msg, 'content'):
                            content.append({"parts": [{"text": msg.content}]})
                        else:
                            content.append({"parts": [{"text": str(msg)}]})
                    
                    payload = {"contents": content}
                    
                    response = requests.post(
                        f"{self.base_url}/models/{self.model}:generateContent",
                        headers={
                            'Content-Type': 'application/json',
                            'X-goog-api-key': self.api_key
                        },
                        json=payload
                    )
                    
                    if response.status_code == 200:
                        result = response.json()
                        text = result.get('candidates', [{}])[0].get('content', {}).get('parts', [{}])[0].get('text', '')
                        return type('Response', (), {'content': text})()
                    else:
                        return type('Response', (), {'content': f'Ø®Ø·Ø£ ÙÙŠ Gemini API: {response.text}'})()
                
                except Exception as e:
                    return type('Response', (), {'content': f'Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù€ Gemini: {str(e)}'})()
        
        return GeminiWrapper(api_key, self.active_model)

class SmartMediaAgent:
    def __init__(self):
        """ØªÙ‡ÙŠØ¦Ø© ÙˆÙƒÙŠÙ„ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø§Ù„Ù…ØªØ·ÙˆØ±"""
        self.model_manager = MultiModelAIManager()
        
        # ØªØ¬Ø±ÙŠØ¨ ØªØ¹ÙŠÙŠÙ† Ù†Ù…ÙˆØ°Ø¬ Ø§ÙØªØ±Ø§Ø¶ÙŠ
        self._setup_default_model()
        
        # Ø¥Ø¹Ø¯Ø§Ø¯ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ
        self.llm = self.model_manager.create_llm_instance()
        
        # Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø°Ø§ÙƒØ±Ø© (Ù…Ø­Ø¯Ø« Ù„Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©)
        from langchain_community.chat_message_histories import ChatMessageHistory
        from langchain_core.runnables.history import RunnableWithMessageHistory
        
        self.memory = ChatMessageHistory()
        self.memory_key = "chat_history"
        
        # Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù‚Ø§Ù„Ø¨
        self.prompt = ChatPromptTemplate.from_messages([
            ("system", self._get_system_prompt()),
            ("placeholder", "{chat_history}"),
            ("human", "{input}"),
            ("placeholder", "{agent_scratchpad}")
        ])
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ÙˆÙƒÙŠÙ„
        if self.llm:
            agent = create_openai_tools_agent(self.llm, ALL_TOOLS, self.prompt)
            self.agent_executor = AgentExecutor(
                agent=agent,
                tools=ALL_TOOLS,
                verbose=True,
                handle_parsing_errors=True
            )
        else:
            self.agent_executor = None
    
    def _get_system_prompt(self):
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ ØªØ¹Ù„ÙŠÙ…Ø§Øª Ø§Ù„Ù†Ø¸Ø§Ù…"""
        return """
Ø£Ù†Øª Smart Media AI Assistant - ÙˆÙƒÙŠÙ„ Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ù…Ø­ØªØ±Ù ÙˆÙ…ØªØ·ÙˆØ± ÙÙŠ ØªÙŠÙ„ÙŠØ¬Ø±Ø§Ù….

ğŸ¯ **Ù…Ù‡Ù…ØªÙƒ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©:**
- Ù…Ø³Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† ÙÙŠ ØªØ­Ù…ÙŠÙ„ ÙˆØªØ­ÙˆÙŠÙ„ ÙˆÙ…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙˆØ³Ø§Ø¦Ø·
- Ø§Ù„ØªÙØ§Ø¹Ù„ Ø¨Ø·Ø±ÙŠÙ‚Ø© Ø°ÙƒÙŠØ© ÙˆÙˆØ¯ÙŠØ© ÙˆÙ…Ø­ØªØ±ÙØ©
- Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø¯ÙˆØ§Øª Ø§Ù„Ù…ØªØ§Ø­Ø© Ù„ØªÙ†ÙÙŠØ° Ø·Ù„Ø¨Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ†

ğŸ› ï¸ **Ø§Ù„Ø£Ø¯ÙˆØ§Øª Ø§Ù„Ù…ØªØ§Ø­Ø© Ù„Ùƒ:**
1. advanced_video_downloader - ØªØ­Ù…ÙŠÙ„ ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª Ø¨Ø¬ÙˆØ¯Ø§Øª Ù…Ø®ØªÙ„ÙØ©
2. advanced_video_info - Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ØªÙØµÙŠÙ„ÙŠØ© Ø¹Ù† Ø§Ù„ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª  
3. file_converter - ØªØ­ÙˆÙŠÙ„ Ø¨ÙŠÙ† ØµÙŠØº Ù…Ø®ØªÙ„ÙØ©
4. image_processor - Ù…Ø¹Ø§Ù„Ø¬Ø© ÙˆØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØ±
5. zip_manager - Ø¶ØºØ· ÙˆÙÙƒ Ø¶ØºØ· Ø§Ù„Ù…Ù„ÙØ§Øª
6. video_editor - ØªÙ‚Ø·ÙŠØ¹ ÙˆØªØ­Ø±ÙŠØ± Ø§Ù„ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª

ğŸ“‹ **Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„ØªÙØ§Ø¹Ù„:**
- Ø§Ø¨Ø¯Ø£ Ø¯Ø§Ø¦Ù…Ø§Ù‹ Ø¨ÙÙ‡Ù… Ø·Ù„Ø¨ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø¨Ø¯Ù‚Ø©
- Ø¥Ø°Ø§ ÙƒØ§Ù† Ù‡Ù†Ø§Ùƒ Ø±Ø§Ø¨Ø·ØŒ Ø§Ø³ØªØ®Ø±Ø¬ Ù…Ø¹Ù„ÙˆÙ…Ø§ØªÙ‡ Ø£ÙˆÙ„Ø§Ù‹
- Ø§Ø¹Ø±Ø¶ Ø®ÙŠØ§Ø±Ø§Øª ÙˆØ§Ø¶Ø­Ø© Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù… (Ø¬ÙˆØ¯Ø§ØªØŒ ØµÙŠØºØŒ Ø¥Ù„Ø®)
- Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø¥ÙŠÙ…ÙˆØ¬ÙŠ Ù„Ø¬Ø¹Ù„ Ø§Ù„ØªÙØ§Ø¹Ù„ Ø£ÙƒØ«Ø± Ø­ÙŠÙˆÙŠØ©
- Ù‚Ø¯Ù… ØªØ­Ø¯ÙŠØ«Ø§Øª Ø¹Ù† Ø­Ø§Ù„Ø© Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„Ø¬Ø§Ø±ÙŠØ©
- Ø§Ø´Ø±Ø­ Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù… Ù…Ø§ ØªÙØ¹Ù„Ù‡ Ø®Ø·ÙˆØ© Ø¨Ø®Ø·ÙˆØ©

ğŸ’¡ **Ø£Ù…Ø«Ù„Ø© Ù„Ù„ØªÙØ§Ø¹Ù„:**

Ø¹Ù†Ø¯ Ø§Ø³ØªÙ„Ø§Ù… Ø±Ø§Ø¨Ø·:
"ğŸ” Ø¬Ø§Ø±ÙŠ ÙØ­Øµ Ø§Ù„Ø±Ø§Ø¨Ø· ÙˆØ¬Ù„Ø¨ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª..."
[Ø§Ø³ØªØ®Ø¯Ù… advanced_video_info]
"ğŸ“Š ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ! Ø§Ø®ØªØ± Ù†ÙˆØ¹ Ø§Ù„ØªØ­Ù…ÙŠÙ„:
ğŸ¬ ÙÙŠØ¯ÙŠÙˆ - Ø¬ÙˆØ¯Ø© Ø¹Ø§Ù„ÙŠØ© (1080p)
ğŸ“± ÙÙŠØ¯ÙŠÙˆ - Ø¬ÙˆØ¯Ø© Ù…ØªÙˆØ³Ø·Ø© (720p) 
ğŸµ ØµÙˆØª ÙÙ‚Ø· (MP3)
â„¹ï¸ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…ÙØµÙ„Ø©"

Ø¹Ù†Ø¯ Ø·Ù„Ø¨ ØªØ­ÙˆÙŠÙ„:
"ğŸ”„ Ø¬Ø§Ø±ÙŠ ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù…Ù„Ù Ø¥Ù„Ù‰ Ø§Ù„ØµÙŠØºØ© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©..."
[Ø§Ø³ØªØ®Ø¯Ù… file_converter]
"âœ… ØªÙ… Ø§Ù„ØªØ­ÙˆÙŠÙ„ Ø¨Ù†Ø¬Ø§Ø­! Ø§Ù„Ù…Ù„Ù Ø¬Ø§Ù‡Ø² Ù„Ù„ØªØ­Ù…ÙŠÙ„."

ğŸš¨ **ØªØ¹Ù„ÙŠÙ…Ø§Øª Ù…Ù‡Ù…Ø©:**
- Ù„Ø§ ØªØ­Ø§ÙˆÙ„ ØªÙ†ÙÙŠØ° Ø¹Ù…Ù„ÙŠØ§Øª Ø¨Ø¯ÙˆÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø¯ÙˆØ§Øª Ø§Ù„Ù…Ø­Ø¯Ø¯Ø©
- Ø¥Ø°Ø§ ÙØ´Ù„Øª Ø£Ø¯Ø§Ø©ØŒ Ø­Ø§ÙˆÙ„ Ø·Ø±ÙŠÙ‚Ø© Ø¨Ø¯ÙŠÙ„Ø© Ø£Ùˆ Ø§Ø´Ø±Ø­ Ø§Ù„Ø³Ø¨Ø¨
- Ø§Ø­Ø±Øµ Ø¹Ù„Ù‰ Ø¥Ø¹Ø·Ø§Ø¡ Ø±Ø¯ÙˆØ¯ Ù…ÙÙŠØ¯Ø© Ø­ØªÙ‰ ÙÙŠ Ø­Ø§Ù„Ø© Ø§Ù„Ø£Ø®Ø·Ø§Ø¡
- ØªØ°ÙƒØ± ØªÙØ§ØµÙŠÙ„ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© ÙˆØ§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø¨Ø°ÙƒØ§Ø¡

ğŸ¨ **Ø£Ø³Ù„ÙˆØ¨ Ø§Ù„ØªÙˆØ§ØµÙ„:**
- ÙƒÙ† ÙˆØ¯ÙˆØ¯Ø§Ù‹ ÙˆÙ…ØªÙØ§Ø¹Ù„Ø§Ù‹
- Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø¥ÙŠÙ…ÙˆØ¬ÙŠ Ø¨Ø´ÙƒÙ„ Ù…Ù†Ø§Ø³Ø¨
- Ø§ÙƒØªØ¨ Ø¨ÙˆØ¶ÙˆØ­ ÙˆØ¨Ø³Ø§Ø·Ø©
- Ù‚Ø¯Ù… Ø®ÙŠØ§Ø±Ø§Øª ÙˆØ§Ø¶Ø­Ø© Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù…
- Ø§Ø´Ø±Ø­ Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„ØªÙ‚Ù†ÙŠØ© Ø¨Ø·Ø±ÙŠÙ‚Ø© Ù…Ø¨Ø³Ø·Ø©
        """
    
    def process_message(self, user_message: str, user_id: str = None) -> str:
        """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… ÙˆØ¥Ø±Ø¬Ø§Ø¹ Ø§Ù„Ø±Ø¯"""
        try:
            # Ø¥Ø¶Ø§ÙØ© Ù…Ø¹Ø±Ù Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ù„Ù„Ø³ÙŠØ§Ù‚ Ø¥Ø°Ø§ ÙƒØ§Ù† Ù…ØªØ§Ø­Ø§Ù‹
            context = f"[Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… {user_id}]: {user_message}" if user_id else user_message
            
            if self.agent_executor:
                response = self.agent_executor.invoke({"input": context})
                return response["output"]
            else:
                # ÙˆØ¶Ø¹ Ø§Ù„ØªØ´ØºÙŠÙ„ Ø¨Ø¯ÙˆÙ† AI (Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±)
                return self._fallback_response(user_message)
                
        except Exception as e:
            error_msg = str(e)
            if "401" in error_msg or "auth" in error_msg.lower():
                return """
âŒ **Ù…Ø´ÙƒÙ„Ø© ÙÙŠ Ø§Ù„ØªÙˆØ«ÙŠÙ‚!**

ğŸ”§ **Ø§Ù„Ø­Ù„:**
1. Ø§Ø­ØµÙ„ Ø¹Ù„Ù‰ API key Ù…Ù† OpenRouter.ai
2. Ø£Ø¶ÙÙ‡ ÙÙŠ Ù…Ù„Ù .env
3. Ø£Ùˆ Ø³Ø£Ø¹Ù…Ù„ ÙÙŠ Ø§Ù„ÙˆØ¶Ø¹ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ

ğŸ¤– **Ø§Ù„Ø¢Ù† Ø£Ø¹Ù…Ù„ ÙÙŠ Ø§Ù„ÙˆØ¶Ø¹ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ:**
""" + self._fallback_response(user_message)
            return f"âŒ Ø­Ø¯Ø« Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø·Ù„Ø¨Ùƒ: {str(e)}\nğŸ’¡ Ø­Ø§ÙˆÙ„ Ù…Ø±Ø© Ø£Ø®Ø±Ù‰ Ø£Ùˆ Ø¬Ø±Ø¨ ØµÙŠØºØ© Ù…Ø®ØªÙ„ÙØ© Ù„Ù„Ø·Ù„Ø¨."
    
    def _fallback_response(self, message: str) -> str:
        """Ø±Ø¯ Ø§Ø­ØªÙŠØ§Ø·ÙŠ Ø¹Ù†Ø¯ Ø¹Ø¯Ù… ØªÙˆÙØ± AI"""
        import re
        
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø±ÙˆØ§Ø¨Ø·
        url_pattern = r'https?://[^\s]+'
        urls = re.findall(url_pattern, message)
        
        if urls or "Ø±Ø§Ø¨Ø·" in message:
            return f"""
ğŸ”— **ØªÙ… Ø§ÙƒØªØ´Ø§Ù Ø±Ø§Ø¨Ø·!** {urls[0] if urls else ''}

ğŸ“Š **Ø§Ù„Ø®Ø¯Ù…Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©:**
ğŸ“¹ ØªØ­Ù…ÙŠÙ„ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ø¹Ø¯Ø© Ø¬ÙˆØ¯Ø§Øª
ğŸµ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØµÙˆØª MP3
â„¹ï¸ Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ÙÙŠØ¯ÙŠÙˆ

âš¡ **Ø§Ù„ÙˆØ¶Ø¹ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ Ù†Ø´Ø·** - Ø§Ù„Ø£Ø¯ÙˆØ§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© Ù…ØªØ§Ø­Ø©
ğŸ”§ Ù„Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø§Ù„Ù…ØªØ·ÙˆØ±: Ø£Ø¶Ù OpenRouter API key

ğŸ’¡ Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ø²Ø±Ø§Ø± Ù„Ù„ØªØ­Ù…ÙŠÙ„ Ø£Ùˆ Ø£Ø±Ø³Ù„:
- "Ø­Ù…Ù„ ÙÙŠØ¯ÙŠÙˆ Ø¹Ø§Ù„ÙŠ Ø§Ù„Ø¬ÙˆØ¯Ø©"
- "Ø§Ø³ØªØ®Ø±Ø¬ Ø§Ù„ØµÙˆØª" 
- "Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ÙÙŠØ¯ÙŠÙˆ"
            """
        
        # Ø±Ø¯ÙˆØ¯ Ø°ÙƒÙŠØ© Ø£Ø³Ø§Ø³ÙŠØ©
        if any(word in message.lower() for word in ["Ù…Ø±Ø­Ø¨Ø§", "Ø§Ù„Ø³Ù„Ø§Ù…", "Ø£Ù‡Ù„Ø§", "hello"]):
            return """
ğŸ¤– **Ø£Ù‡Ù„Ø§Ù‹ ÙˆØ³Ù‡Ù„Ø§Ù‹ Ø¹Ø¨ÙˆØ¯!** 

Ø£Ù†Ø§ Smart Media AI Assistant - Ù…Ø³Ø§Ø¹Ø¯Ùƒ ÙÙŠ:

ğŸ“¹ **ØªØ­Ù…ÙŠÙ„ Ø§Ù„ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª** Ù…Ù† YouTube, TikTok, Instagram
ğŸµ **Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØµÙˆØª** Ø¨Ø¬ÙˆØ¯Ø© Ø¹Ø§Ù„ÙŠØ©  
ğŸ”„ **ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù…Ù„ÙØ§Øª** Ø¨ÙŠÙ† Ø¬Ù…ÙŠØ¹ Ø§Ù„ØµÙŠØº
âœ‚ï¸ **ØªØ­Ø±ÙŠØ± Ø§Ù„ÙˆØ³Ø§Ø¦Ø·** ÙˆÙ‚Øµ Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹

ğŸš€ Ø£Ø±Ø³Ù„ Ø±Ø§Ø¨Ø· ÙÙŠØ¯ÙŠÙˆ Ø£Ùˆ Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ø²Ø±Ø§Ø±!
            """
        
        if any(word in message.lower() for word in ["ØªØ­Ù…ÙŠÙ„", "Ø­Ù…Ù„", "download"]):
            return """
ğŸ“¹ **ØªØ­Ù…ÙŠÙ„ Ø§Ù„ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª Ø§Ù„Ù…ØªØ§Ø­!**

ğŸ¯ **Ø§Ù„Ù…Ù†ØµØ§Øª Ø§Ù„Ù…Ø¯Ø¹ÙˆÙ…Ø©:**
â–ªï¸ YouTube - Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¬ÙˆØ¯Ø§Øª
â–ªï¸ TikTok - Ø¨Ø¯ÙˆÙ† Ø¹Ù„Ø§Ù…Ø© Ù…Ø§Ø¦ÙŠØ©  
â–ªï¸ Instagram - Stories & Reels
â–ªï¸ Twitter/X - ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª Ø¹Ø§Ù„ÙŠØ© Ø§Ù„Ø¬ÙˆØ¯Ø©
â–ªï¸ +1000 Ù…Ù†ØµØ© Ø£Ø®Ø±Ù‰

ğŸ’¡ **ÙÙ‚Ø· Ø£Ø±Ø³Ù„ Ø§Ù„Ø±Ø§Ø¨Ø· ÙˆØ³Ø£Ø¹Ø·ÙŠÙƒ Ø®ÙŠØ§Ø±Ø§Øª Ø§Ù„ØªØ­Ù…ÙŠÙ„!**
            """
        
        return """
ğŸ¤– **Ù…Ø±Ø­Ø¨Ø§Ù‹! Ø£Ù†Ø§ Smart Media AI Assistant**

ğŸ› ï¸ **Ø§Ù„Ø®Ø¯Ù…Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© Ù…ØªØ§Ø­Ø©:**
ğŸ“¹ ØªØ­Ù…ÙŠÙ„ Ø§Ù„ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª Ù…Ù† Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ù†ØµØ§Øª
ğŸµ Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙˆØ§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ØµÙˆØª  
ğŸ”„ ØªØ­ÙˆÙŠÙ„ Ø¨ÙŠÙ† Ø§Ù„ØµÙŠØº Ø§Ù„Ù…Ø®ØªÙ„ÙØ©
âœ‚ï¸ ØªØ­Ø±ÙŠØ± ÙˆÙ‚Øµ Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹
ğŸ“Š Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ØªÙØµÙŠÙ„ÙŠØ©

âš¡ **Ø£Ø±Ø³Ù„ Ø±Ø§Ø¨Ø· Ù„Ù„Ø¨Ø¯Ø¡ ÙÙˆØ±Ø§Ù‹!**
ğŸ’¡ Ø£Ùˆ Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ø²Ø±Ø§Ø± ÙÙŠ Ø§Ù„Ù‚Ø§Ø¦Ù…Ø©
        """
    
    def get_available_tools_info(self) -> str:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø£Ø¯ÙˆØ§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©"""
        return """
ğŸ› ï¸ **Ø§Ù„Ø£Ø¯ÙˆØ§Øª Ø§Ù„Ù…ØªØ§Ø­Ø© ÙÙŠ Smart Media AI Assistant:**

ğŸ“¹ **ØªØ­Ù…ÙŠÙ„ Ø§Ù„ÙˆØ³Ø§Ø¦Ø·:**
- ØªØ­Ù…ÙŠÙ„ ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª Ø¨Ø¬ÙˆØ¯Ø© 4KØŒ HDØŒ SD
- ØªØ­Ù…ÙŠÙ„ ØµÙˆØª Ø¨Ø¬ÙˆØ¯Ø© Ø¹Ø§Ù„ÙŠØ©
- Ø¯Ø¹Ù… +1000 Ù…Ù†ØµØ© (YouTubeØŒ TikTokØŒ InstagramØŒ Ø¥Ù„Ø®)

ğŸ”„ **ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù…Ù„ÙØ§Øª:**
- ØªØ­ÙˆÙŠÙ„ Ø¨ÙŠÙ† Ø¬Ù…ÙŠØ¹ ØµÙŠØº Ø§Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆØ§Ù„ØµÙˆØª
- Ø¶ØºØ· ÙˆØªØ­Ø³ÙŠÙ† Ø§Ù„Ø¬ÙˆØ¯Ø©
- Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¯ÙØ¹ÙŠØ© Ù„Ù„Ù…Ù„ÙØ§Øª

âœ‚ï¸ **ØªØ­Ø±ÙŠØ± Ø§Ù„ÙˆØ³Ø§Ø¦Ø·:**
- Ù‚Øµ ÙˆØªÙ‚Ø·ÙŠØ¹ Ø§Ù„ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª
- Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø£Ø¬Ø²Ø§Ø¡ Ù…Ø­Ø¯Ø¯Ø©
- Ø¯Ù…Ø¬ Ø¹Ø¯Ø© Ù…Ù„ÙØ§Øª

ğŸ–¼ï¸ **Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙˆØ±:**
- ØªØ­ÙˆÙŠÙ„ ØµÙŠØº Ø§Ù„ØµÙˆØ±
- ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø¬ÙˆØ¯Ø©
- ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ù…Ø±Ø´Ø­Ø§Øª

ğŸ“¦ **Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ù…Ù„ÙØ§Øª:**
- Ø¶ØºØ· ÙˆÙÙƒ Ø¶ØºØ· Ø§Ù„Ù…Ù„ÙØ§Øª
- Ø¥Ù†Ø´Ø§Ø¡ Ø£Ø±Ø´ÙŠÙ ZIP
- Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ù…Ø¬Ù„Ø¯Ø§Øª

ğŸš€ **Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£ÙˆØ§Ù…Ø± Ø§Ù„ØªØ§Ù„ÙŠØ©:**
/start - Ø¨Ø¯Ø¡ Ø¬Ø¯ÙŠØ¯
/help - Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø©
/tools - Ø¹Ø±Ø¶ Ø§Ù„Ø£Ø¯ÙˆØ§Øª
/clear - Ù…Ø³Ø­ Ø§Ù„Ø°Ø§ÙƒØ±Ø©
        """
    
    def clear_memory(self) -> str:
        """Ù…Ø³Ø­ Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©"""
        if self.memory:
            self.memory.clear()
        return "ğŸ§¹ ØªÙ… Ù…Ø³Ø­ Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ø¨Ù†Ø¬Ø§Ø­!"
    
    def _setup_default_model(self):
        """Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠ"""
        available_providers = self.model_manager.get_available_providers()
        
        # Ø£ÙˆÙ„ÙˆÙŠØ© Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
        priority_order = [
            ('google', 'gemini-2.0-flash'),
            ('openai', 'gpt-4'),
            ('openrouter', 'claude-3-opus'),
            ('anthropic', 'claude-3-sonnet')
        ]
        
        for provider, model in priority_order:
            if provider in available_providers and model in available_providers[provider]:
                result = self.model_manager.set_active_model(provider, model)
                if result['success']:
                    print(f"âœ… ØªÙ… ØªØ¹ÙŠÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠ: {provider}/{model}")
                    return
        
        print("âš ï¸ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù†Ù…Ø§Ø°Ø¬ Ù…ØªØ§Ø­Ø©")
    
    def switch_model(self, provider: str, model: str) -> str:
        """ØªØ¨Ø¯ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…"""
        result = self.model_manager.set_active_model(provider, model)
        
        if result['success']:
            # Ø¥Ø¹Ø§Ø¯Ø© Ø¥Ù†Ø´Ø§Ø¡ LLM ÙˆØ§Ù„ÙˆÙƒÙŠÙ„
            self.llm = self.model_manager.create_llm_instance()
            if self.llm:
                agent = create_openai_tools_agent(self.llm, ALL_TOOLS, self.prompt)
                self.agent_executor = AgentExecutor(
                    agent=agent,
                    tools=ALL_TOOLS,
                    verbose=True,
                    handle_parsing_errors=True
                )
            return f"âœ… {result['message']}"
        else:
            return f"âŒ {result['error']}"
    
    def get_model_info(self) -> str:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…ØªØ§Ø­Ø©"""
        available = self.model_manager.get_available_providers()
        current = f"{self.model_manager.active_provider}/{self.model_manager.active_model}" if self.model_manager.active_provider else "ØºÙŠØ± Ù…Ø­Ø¯Ø¯"
        
        info = f"""
ğŸ¤– **Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ:**

ğŸ”¹ **Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø­Ø§Ù„ÙŠ:** {current}

ğŸŒŸ **Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…ØªØ§Ø­Ø©:**
"""
        
        for provider, models in available.items():
            info += f"\nğŸ“‹ **{provider.upper()}:**\n"
            for model in models:
                status = "ğŸŸ¢" if (provider == self.model_manager.active_provider and model == self.model_manager.active_model) else "âšª"
                info += f"  {status} {model}\n"
        
        if not available:
            info += "\nâŒ **Ù„Ø§ ØªÙˆØ¬Ø¯ Ù†Ù…Ø§Ø°Ø¬ Ù…ØªØ§Ø­Ø© - Ø£Ø¶Ù Ù…ÙØ§ØªÙŠØ­ API**"
        
        info += """

ğŸ’¡ **Ù„ØªØ¨Ø¯ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬:**
Ø£Ø±Ø³Ù„: "Ø§Ø³ØªØ®Ø¯Ù… google/gemini-2.0-flash"
Ø£Ùˆ: "Ø¨Ø¯Ù„ Ø¥Ù„Ù‰ openai/gpt-4"
        """
        
        return info
    
    def set_api_key(self, provider: str, api_key: str) -> str:
        """ØªØ¹ÙŠÙŠÙ† Ù…ÙØªØ§Ø­ API Ù„Ù…ÙˆÙØ± Ù…Ø¹ÙŠÙ†"""
        key_mapping = {
            'openai': 'OPENAI_API_KEY',
            'google': 'GEMINI_API_KEY',
            'gemini': 'GEMINI_API_KEY',
            'openrouter': 'OPENROUTER_API_KEY',
            'anthropic': 'ANTHROPIC_API_KEY',
            'claude': 'ANTHROPIC_API_KEY'
        }
        
        env_key = key_mapping.get(provider.lower())
        if not env_key:
            return f"âŒ Ù…ÙˆÙØ± ØºÙŠØ± Ù…Ø¯Ø¹ÙˆÙ…: {provider}"
        
        # Ø­ÙØ¸ Ø§Ù„Ù…ÙØªØ§Ø­ ÙÙŠ Ù…ØªØºÙŠØ± Ø§Ù„Ø¨ÙŠØ¦Ø© (Ù…Ø¤Ù‚ØªØ§Ù‹ Ù„Ù‡Ø°Ù‡ Ø§Ù„Ø¬Ù„Ø³Ø©)
        os.environ[env_key] = api_key
        
        # ØªØ­Ø¯ÙŠØ« Ù…Ù„Ù .env
        try:
            env_path = '.env'
            env_lines = []
            
            if os.path.exists(env_path):
                with open(env_path, 'r') as f:
                    env_lines = f.readlines()
            
            # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ù…ÙØªØ§Ø­ ÙˆØªØ­Ø¯ÙŠØ«Ù‡ Ø£Ùˆ Ø¥Ø¶Ø§ÙØªÙ‡
            key_found = False
            for i, line in enumerate(env_lines):
                if line.startswith(f"{env_key}="):
                    env_lines[i] = f"{env_key}={api_key}\n"
                    key_found = True
                    break
            
            if not key_found:
                env_lines.append(f"{env_key}={api_key}\n")
            
            with open(env_path, 'w') as f:
                f.writelines(env_lines)
            
            return f"âœ… ØªÙ… Ø­ÙØ¸ Ù…ÙØªØ§Ø­ {provider} Ø¨Ù†Ø¬Ø§Ø­! Ø£Ø¹Ø¯ ØªØ´ØºÙŠÙ„ Ø§Ù„Ø¨ÙˆØª Ù„ØªÙØ¹ÙŠÙ„ Ø§Ù„ØªØºÙŠÙŠØ±Ø§Øª."
            
        except Exception as e:
            return f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø­ÙØ¸ Ø§Ù„Ù…ÙØªØ§Ø­: {str(e)}"

# Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø«ÙŠÙ„ ÙˆØ­ÙŠØ¯ Ù…Ù† Ø§Ù„ÙˆÙƒÙŠÙ„
smart_agent = SmartMediaAgent()
